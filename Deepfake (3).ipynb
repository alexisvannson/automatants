{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFQiTRxteIuZ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "OrUHe8sLeIRe",
        "outputId": "07f5b01f-1b3e-4180-aef4-c8083812ff77"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9590050a-3613-42a8-9af3-7b951fb062fa\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9590050a-3613-42a8-9af3-7b951fb062fa\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (1).json\n",
            "User uploaded file \"kaggle (1).json\" with length 69 bytes\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "        name=fn, length=len(uploaded[fn])))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "VD-xuMEueMnT",
        "outputId": "dd7d727f-8250-4125-c502-9c887f716a25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.12)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.7)\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "mp192LXieP6s",
        "outputId": "c9e45dd5-d3ba-4489-e733-7f656cb28fe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory /content/automathon-deepfake already contains data.\n"
          ]
        }
      ],
      "source": [
        "!if [ ! -d \"/content/automathon-deepfake\" ]; then kaggle competitions download -c automathon-deepfake -p /content/automathon-deepfake; unzip /content/automathon-deepfake/automathon-deepfake.zip -d /content/automathon-deepfake; else echo \"Directory /content/automathon-deepfake already contains data.\"; fi\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "xAB3Aex4eTX3",
        "outputId": "13a4fa7c-d1b1-4ed3-ab2e-aabd6a4a1a91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vv-YAeJVef3V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d5b4f3e2-3d7e-4e07-eabf-a9b074b3182f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat '/content/automathon-deepfake/dataset/experimental_dataset/metadata.json': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!mv /content/automathon-deepfake/dataset/experimental_dataset/metadata.json /content/automathon-deepfake/dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/automathon-deepfake\n",
        "%mkdir frames"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "hgCDRKmMufUZ",
        "outputId": "82428fbc-4e6d-460b-d740-7ca6b4d2ebf4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/automathon-deepfake\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torchvision.transforms import Compose, ToPILImage, Resize, CenterCrop, ToTensor, Normalize\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms.functional import to_tensor, normalize\n",
        "from torchvision.io import read_video\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from PIL import Image\n",
        "from torchvision.utils import save_image\n",
        "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
        "import torchvision.transforms.functional as F\n",
        "from torchsummary import summary\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "GymqlA4au4Mf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_VIDEO_PATH = \"/content/automathon-deepfake/dataset/experimental_dataset\"\n",
        "DATASET_METADATA_PATH = \"/content/automathon-deepfake/dataset/metadata.json\"\n",
        "FRAME_SAVE_PATH = \"/content/automathon-deepfake/frames\"\n",
        "FRAME_RATE = 1  # Frame rate to sample (e.g., 1 frame per second)\n",
        "\n",
        "# Make sure the frame save directory exists\n",
        "os.makedirs(FRAME_SAVE_PATH, exist_ok=True)\n",
        "\n",
        "# Load video metadata\n",
        "df_labels = pd.read_json(DATASET_METADATA_PATH, orient='index')\n",
        "df_labels.reset_index(inplace=True)\n",
        "df_labels.columns = ['Filename', 'Label']\n",
        "df_labels['label_value'] = np.where(df_labels['Label'] == 'real', 1, 0)"
      ],
      "metadata": {
        "id": "MN1SVNuGuUUE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, dataframe, root_dir, sequence_length=10, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataframe (DataFrame): DataFrame containing video filenames and labels.\n",
        "            root_dir (str): Directory path where video files are stored.\n",
        "            sequence_length (int): Number of frames to extract from each video.\n",
        "            transform (callable, optional): Optional transform to be applied on a frame.\n",
        "        \"\"\"\n",
        "        self.dataframe = dataframe\n",
        "        self.root_dir = root_dir\n",
        "        self.sequence_length = sequence_length\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_filename = self.dataframe.iloc[idx]['Filename']\n",
        "        video_path = os.path.join(self.root_dir, video_filename)\n",
        "        label = self.dataframe.iloc[idx]['label_value']\n",
        "        # Read video and extract frames\n",
        "        frames, _, _ = read_video(video_path, pts_unit='sec', start_pts=0, end_pts=10, output_format='TCHW')\n",
        "        total_frames = len(frames)\n",
        "        frame_indices = torch.linspace(0, total_frames - 1, steps=self.sequence_length).long()\n",
        "        selected_frames = frames[frame_indices]\n",
        "\n",
        "        processed_frames = []\n",
        "        for frame in selected_frames:\n",
        "            if self.transform:\n",
        "                frame = self.transform(frame)\n",
        "            processed_frames.append(frame)\n",
        "\n",
        "        frames_tensor = torch.stack(processed_frames)\n",
        "        return frames_tensor, label\n",
        "\n",
        "# Example of setting up the dataset and dataloader with transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),  # Necessary to convert raw video frame to PIL Image for some transformations\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((224,224)),\n",
        "    transforms.ToTensor(),  # Convert the PIL Image to a tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the tensor\n",
        "])\n",
        "\n",
        "dataset = VideoDataset(df_labels, DATASET_VIDEO_PATH, transform=transform)\n",
        "loader = DataLoader(dataset, batch_size=1, shuffle=True)"
      ],
      "metadata": {
        "id": "c0aXr6_5vHeI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Assuming 'df_labels' is your DataFrame containing video filenames and their labels\n",
        "# Shuffle the DataFrame\n",
        "df_labels = df_labels.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Define split sizes\n",
        "train_size = int(0.7 * len(df_labels))\n",
        "val_size = int(0.15 * len(df_labels))\n",
        "test_size = len(df_labels) - train_size - val_size\n",
        "\n",
        "# Split the DataFrame into train, validation, and test sets\n",
        "train_df = df_labels[:train_size]\n",
        "val_df = df_labels[train_size:train_size + val_size]\n",
        "test_df = df_labels[train_size + val_size:]\n",
        "\n",
        "# Define a transform pipeline\n",
        "transform = Compose([\n",
        "    ToPILImage(),  # Convert raw video frame to PIL Image for transformations\n",
        "    Resize((256, 256)),\n",
        "    CenterCrop((224, 224)),\n",
        "    ToTensor(),  # Convert the PIL Image to a tensor\n",
        "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the tensor\n",
        "])\n",
        "\n",
        "# Create datasets for each set\n",
        "train_dataset = VideoDataset(train_df, DATASET_VIDEO_PATH, transform=transform)\n",
        "val_dataset = VideoDataset(val_df, DATASET_VIDEO_PATH, transform=transform)\n",
        "test_dataset = VideoDataset(test_df, DATASET_VIDEO_PATH, transform=transform)\n",
        "\n",
        "# Create data loaders for each set\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Now you can use these loaders in your training loop\n"
      ],
      "metadata": {
        "id": "dnv3rxxGvTji"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Assuming train_loader is already defined\n",
        "data_loader = train_loader  # You can replace this with val_loader or test_loader as needed\n",
        "\n",
        "# Fetch and print the shape of one batch of data and labels from the specified DataLoader\n",
        "for data, labels in data_loader:\n",
        "    print(f\"Data shape: {data.shape}\")  # Shape of the video frames tensor\n",
        "    print(f\"Labels shape: {labels.shape}\")  # Shape of the labels tensor\n",
        "    break  # Only look at the first batch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ZAHx0Z4h6xZv",
        "outputId": "de895d3a-036e-4bfe-cdb7-55bd3ace5260"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape: torch.Size([1, 10, 3, 224, 224])\n",
            "Labels shape: torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class VideoCNN3D(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VideoCNN3D, self).__init__()\n",
        "        # 3D convolutional layer with input channels = 3 (RGB)\n",
        "        self.conv1 = nn.Conv3d(3, 32, kernel_size=(3, 3, 3), padding=1)\n",
        "        self.pool1 = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
        "\n",
        "        # Second 3D convolutional layer\n",
        "        self.conv2 = nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=1)\n",
        "        self.pool2 = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
        "\n",
        "        # Third 3D convolutional layer\n",
        "        self.conv3 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=1)\n",
        "        self.pool3 = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
        "\n",
        "        # Flattening and dense layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(128 * 10 * 28 * 28, 512)  # Adjust size according to the output of the last pool layer\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(512, 2)  # Assuming binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Invert sequence length and channels dimensions\n",
        "        x = input_tensor.permute(0, 2, 1, 3, 4)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.softmax(x, dim=1)\n",
        "        return torch.argmax(x, dim=1)\n",
        "\n",
        "# Example initialization and forward pass simulation\n",
        "model = VideoCNN3D()\n",
        "print(model)\n",
        "\n",
        "# Simulate a forward pass\n",
        "input_tensor = torch.randn(1, 10 ,3, 224, 224)  # Batch size, Channels, Sequence length, Height, Width\n",
        "output = model(input_tensor)\n",
        "print(\"Output shape:\", output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "N7uRhcG97rCI",
        "outputId": "c525e87f-5e45-4520-b25b-e78e95b43fdd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VideoCNN3D(\n",
            "  (conv1): Conv3d(3, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  (pool1): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  (pool2): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv3): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  (pool3): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (fc1): Linear(in_features=1003520, out_features=512, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n",
            "Output shape: torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class VideoCNN3D(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VideoCNN3D, self).__init__()\n",
        "        # First 3D convolutional layer with input channels = 3 (RGB)\n",
        "        self.conv1 = nn.Conv3d(3, 32, kernel_size=(3, 3, 3), padding=1)\n",
        "        self.pool1 = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
        "\n",
        "        # Second 3D convolutional layer\n",
        "        self.conv2 = nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=1)\n",
        "        self.pool2 = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
        "\n",
        "        # Flattening and dense layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(64 * 5 * 28 * 28, 512)  # Adjust size according to the output of the last pool layer\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(512, 2)  # Assuming binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Invert sequence length and channels dimensions\n",
        "        x = x.permute(0, 2, 1, 3, 4)\n",
        "\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.softmax(x, dim=1)\n",
        "        return torch.argmax(x, dim=1)\n",
        "\n",
        "# Example initialization and forward pass simulation\n",
        "model = VideoCNN3D()\n",
        "print(model)\n",
        "\n",
        "# Simulate a forward pass\n",
        "input_tensor = torch.randn(1, 10, 3, 224, 224)  # Batch size, Sequence length, Channels, Height, Width\n",
        "output = model(input_tensor)\n",
        "print(\"Output shape:\", output.shape)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "id": "OQwLzrTrGHNi",
        "outputId": "552289dd-c066-4b76-e6b1-c3fb318c6337"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VideoCNN3D(\n",
            "  (conv1): Conv3d(3, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  (pool1): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
            "  (pool2): MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (fc1): Linear(in_features=250880, out_features=512, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (1x2007040 and 250880x512)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-55d5c9822538>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Simulate a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Batch size, Sequence length, Channels, Height, Width\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Output shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-55d5c9822538>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x2007040 and 250880x512)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = VideoCNN3D()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "CwjdgvD98z_y",
        "outputId": "86d41b4e-10bb-4223-fb58-743b220cf837"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install av"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "tNUKG0YOy3DS",
        "outputId": "c85f04ae-c310-4444-d163-8cacf6c8a07b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: av in /usr/local/lib/python3.10/dist-packages (12.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs\n",
        "            labels = labels\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        print(f'Training Loss: {epoch_loss:.4f}')\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs = inputs\n",
        "            labels = labels\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        print(f'Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "        # Deep copy the model if it has the best validation loss so far\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            print('Best model saved.')\n",
        "\n",
        "        # Step the scheduler\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "    print('Training complete. Best val Loss: {:4f}'.format(best_loss))\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "# Train the model\n",
        "trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "kqLX8i1C0lKM",
        "outputId": "417e9a65-2fa2-492e-d012-badd0cd9bf69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "----------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmA4tb8seh8H"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "# Load metadata from a JSON file\n",
        "with open(\"/content/automathon-deepfake/dataset/metadata.json\", 'r') as file:\n",
        "    metadata = json.load(file)\n",
        "\n",
        "def extract_frames(video_path, output_folder, video_name, label, every_n_seconds=1):\n",
        "    # Create the output folder if it does not exist\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    clip = VideoFileClip(video_path)\n",
        "    frame_count = int(clip.fps * clip.duration)  # Total number of frames\n",
        "    for i in range(0, frame_count, int(clip.fps * every_n_seconds)):\n",
        "        frame = clip.get_frame(i / clip.fps)\n",
        "        # Frame filename includes video name, label, and frame index\n",
        "        frame_path = os.path.join(output_folder, f\"{video_name}_label_{label}_frame_{i}.png\")\n",
        "        clip.save_frame(frame_path, t=i / clip.fps)\n",
        "\n",
        "# Define base paths\n",
        "base_output_folder = \"/content/automathon-deepfake\"\n",
        "video_folder = \"/content/automathon-deepfake/dataset/experimental_dataset\"\n",
        "frames_folder = os.path.join(base_output_folder, \"frames\")\n",
        "\n",
        "# Loop through all video files in the video folder\n",
        "for video_file in os.listdir(video_folder):\n",
        "    if video_file.endswith('.mp4'):\n",
        "        video_path = os.path.join(video_folder, video_file)\n",
        "        video_name = os.path.splitext(video_file)[0]\n",
        "        label = metadata.get(video_file, 'unknown')  # Default to 'unknown' if not found in metadata\n",
        "        video_output_folder = os.path.join(frames_folder, video_name)  # Output folder for each video\n",
        "\n",
        "        # Extract frames to the specific video folder\n",
        "        extract_frames(video_path, video_output_folder, video_name, label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7i5-FulgWXY"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision numpy matplotlib pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkfnkmDgswkn"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "model = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
        "model.eval()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJwkXnVUkxL-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from torchvision.io import read_image\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "import torch\n",
        "import time\n",
        "\n",
        "def replace_frames_with_faces(image_path, model, device):\n",
        "    # Load the image\n",
        "    image = read_image(image_path).float() / 255  # Normalize the image\n",
        "    image = image.to(device)\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        prediction = model([image])[0]\n",
        "\n",
        "    # Filter results with high confidence and person labels\n",
        "    boxes = prediction['boxes'][prediction['scores'] > 0.8]\n",
        "    labels = prediction['labels'][prediction['scores'] > 0.8]\n",
        "    person_boxes = boxes[labels == 1]\n",
        "\n",
        "    if person_boxes.shape[0] > 0:\n",
        "        # Use the first detected face\n",
        "        xmin, ymin, xmax, ymax = person_boxes[0]\n",
        "        height = ymax - ymin\n",
        "        face_image = image[:, int(ymin):int(ymin + height), int(xmin):int(xmax)]\n",
        "\n",
        "        # Save the face image back to the original path\n",
        "        face_pil = to_pil_image(face_image.cpu())\n",
        "        face_pil.save(image_path)\n",
        "        print(f\"Processed and saved face image for {image_path}\")\n",
        "    else:\n",
        "        print(f\"No faces detected in {image_path}\")\n",
        "\n",
        "def process_and_replace_faces(base_folder, model, device):\n",
        "    for subdir in os.listdir(base_folder):\n",
        "        subdir_path = os.path.join(base_folder, subdir)\n",
        "        if os.path.isdir(subdir_path):  # Check if it is a directory\n",
        "            print(f\"Processing directory: {subdir_path}\")\n",
        "            for image_file in os.listdir(subdir_path):\n",
        "                if image_file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    image_path = os.path.join(subdir_path, image_file)\n",
        "                    replace_frames_with_faces(image_path, model, device)\n",
        "                else:\n",
        "                    print(f\"Skipped non-image file: {image_file}\")\n",
        "        else:\n",
        "            print(f\"Skipped non-directory file: {subdir}\")\n",
        "    print(\"Processing complete.\")\n",
        "\n",
        "# Assuming the model and device setup from previous example\n",
        "model.eval()  # Make sure model is in evaluation mode\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Define your frames directory\n",
        "frames_directory = '/content/automathon-deepfake/frames'\n",
        "process_and_replace_faces(frames_directory,model, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "veb9Awyxn_l-"
      },
      "outputs": [],
      "source": [
        "def replace_frames_with_faces(image_path, model, device):\n",
        "    # Load the image\n",
        "    image = read_image(image_path).float() / 255  # Normalize the image\n",
        "    image = image.to(device)\n",
        "\n",
        "    # Perform inference\n",
        "    with torch.no_grad():\n",
        "        prediction = model([image])[0]\n",
        "\n",
        "    # Filter results with high confidence and person labels\n",
        "    boxes = prediction['boxes'][prediction['scores'] > 0.8]\n",
        "    labels = prediction['labels'][prediction['scores'] > 0.8]\n",
        "    person_boxes = boxes[labels == 1]\n",
        "\n",
        "    if person_boxes.shape[0] > 0:\n",
        "        # Use the first detected face\n",
        "        xmin, ymin, xmax, ymax = person_boxes[0]\n",
        "        height = ymax - ymin\n",
        "        face_image = image[:, int(ymin):int(ymin + height), int(xmin):int(xmax)]\n",
        "\n",
        "        # Save the face image back to the original path\n",
        "        face_pil = to_pil_image(face_image.cpu())\n",
        "        face_pil.save(image_path)\n",
        "        print(f\"Processed and saved face image for {image_path}\")\n",
        "    else:\n",
        "        print(f\"No faces detected in {image_path}\")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVachQoOt1wR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models import resnet18\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gNSOhXDt4-A"
      },
      "outputs": [],
      "source": [
        "class DeepfakeDataset(Dataset):\n",
        "    def __init__(self, img_dir, metadata, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_dir (string): Directory with all the images.\n",
        "            metadata (dict): Dictionary with labels for the images.\n",
        "            transform (callable, optional): Optional transform to be applied on a sample.\n",
        "        \"\"\"\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.metadata = metadata\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.metadata)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = list(self.metadata.keys())[idx]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "        image = read_image(img_path)\n",
        "        label = self.metadata[img_name]\n",
        "        sample = {'image': image, 'label': label}\n",
        "\n",
        "        if self.transform:\n",
        "            sample['image'] = self.transform(sample['image'])\n",
        "\n",
        "        return sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlAdLTlQt-4i"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ConvertImageDtype(torch.float),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3zrv3UFuCtB"
      },
      "outputs": [],
      "source": [
        "# Load metadata\n",
        "with open(\"/content/automathon-deepfake/dataset/metadata.json\", 'r') as file:\n",
        "    metadata = json.load(file)\n",
        "\n",
        "# Convert labels to integers if they're not already\n",
        "for key in metadata:\n",
        "    metadata[key] = 1 if metadata[key] == 'real' else 0\n",
        "\n",
        "img_dir = \"/content/automathon-deepfake/dataset/frames\"\n",
        "dataset = DeepfakeDataset(img_dir=img_dir, metadata=metadata, transform=transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXtzHAISu4RB"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "# Assuming dataset is already created\n",
        "total_size = len(dataset)\n",
        "train_size = int(0.7 * total_size)\n",
        "valid_size = int(0.2 * total_size)\n",
        "test_size = total_size - train_size - valid_size\n",
        "\n",
        "train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCOd5CWLuGZX"
      },
      "outputs": [],
      "source": [
        "model = resnet18(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 2)  # Assuming binary classification\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5A2GO4juLqa"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcLtOA2CuMzt"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "train_losses, valid_losses = [], []\n",
        "best_valid_loss = float('inf')  # Initialize with a very large value\n",
        "patience = 3  # Number of epochs to wait for improvement\n",
        "counter = 0  # Counter for patience\n",
        "\n",
        "num_epochs = 30  # Increase epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    for data in train_loader:\n",
        "        inputs, labels = data['image'], data['label']\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    valid_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for data in valid_loader:\n",
        "            inputs, labels = data['image'], data['label']\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "    # Calculate average losses\n",
        "    train_loss = train_loss / len(train_loader)\n",
        "    valid_loss = valid_loss / len(valid_loader)\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)\n",
        "\n",
        "    print(f'Epoch {epoch+1}: Training Loss: {train_loss:.4f}, Validation Loss: {valid_loss:.4f}')\n",
        "\n",
        "    # Check for improvement in validation loss\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        counter = 0  # Reset counter\n",
        "        # Save the model\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(f'Validation loss has not improved for {patience} epochs. Early stopping...')\n",
        "            break\n",
        "\n",
        "# Plot the losses\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(valid_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_Skn9GodHnR"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision tqdm pillow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGghrN4Ud9es"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from torchvision.transforms import functional as F\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ms0OalgleNAq"
      },
      "outputs": [],
      "source": [
        "def extract_faces_from_video(video_path, output_folder):\n",
        "    from moviepy.editor import VideoFileClip\n",
        "\n",
        "    cap = VideoFileClip(video_path)\n",
        "    frames = []\n",
        "\n",
        "    with tqdm(total=cap.reader.nframes) as pbar:\n",
        "        for frame in cap.iter_frames():\n",
        "            frame = Image.fromarray(frame)\n",
        "            frame_tensor = F.to_tensor(frame).to(device)\n",
        "            outputs = mtcnn([frame_tensor])[0]\n",
        "\n",
        "            for box, score in zip(outputs['boxes'], outputs['scores']):\n",
        "                if score > 0.9:\n",
        "                    x1, y1, x2, y2 = [int(val) for val in box]\n",
        "                    face = frame.crop((x1, y1, x2, y2))\n",
        "                    frames.append(face)\n",
        "\n",
        "            pbar.update(1)\n",
        "\n",
        "    cap.close()\n",
        "\n",
        "    for i, face in enumerate(frames):\n",
        "        face.save(f\"{output_folder}/face_{i}.jpg\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}