{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VE1g3XG0IGFN"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "        name=fn, length=len(uploaded[fn])))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!pip install kaggle"
      ],
      "metadata": {
        "id": "UfH9E-0IIJ-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!if [ ! -d \"/content/automathon-deepfake\" ]; then kaggle competitions download -c automathon-deepfake -p /content/automathon-deepfake; unzip /content/automathon-deepfake/automathon-deepfake.zip -d /content/automathon-deepfake; else echo \"Directory /content/automathon-deepfake already contains data.\"; fi\n",
        "\n"
      ],
      "metadata": {
        "id": "o7mA5bHmIKm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision\n"
      ],
      "metadata": {
        "id": "-ZnOQ_ilIKpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/automathon-deepfake/dataset/experimental_dataset/metadata.json /content/automathon-deepfake/dataset"
      ],
      "metadata": {
        "id": "XsZMAS_9IKrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/automathon-deepfake\n",
        "%mkdir frames"
      ],
      "metadata": {
        "id": "J3uKky7UIKta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torchvision.transforms import Compose, ToPILImage, Resize, CenterCrop, ToTensor, Normalize\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms.functional import to_tensor, normalize\n",
        "from torchvision.io import read_video\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "from PIL import Image\n",
        "from torchvision.utils import save_image\n",
        "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
        "import torchvision.transforms.functional as F\n",
        "from torchsummary import summary\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "mhNudqSfIKvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_VIDEO_PATH = \"/content/automathon-deepfake/dataset/experimental_dataset\"\n",
        "DATASET_METADATA_PATH = \"/content/automathon-deepfake/dataset/metadata.json\"\n",
        "FRAME_SAVE_PATH = \"/content/automathon-deepfake/frames\"\n",
        "FRAME_RATE = 1  # Frame rate to sample (e.g., 1 frame per second)\n",
        "\n",
        "# Make sure the frame save directory exists\n",
        "os.makedirs(FRAME_SAVE_PATH, exist_ok=True)\n",
        "\n",
        "# Load video metadata\n",
        "df_labels = pd.read_json(DATASET_METADATA_PATH, orient='index')\n",
        "df_labels.reset_index(inplace=True)\n",
        "df_labels.columns = ['Filename', 'Label']\n",
        "df_labels['label_value'] = np.where(df_labels['Label'] == 'real', 1, 0)"
      ],
      "metadata": {
        "id": "tGaNTcmBIKyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoDataset(Dataset):\n",
        "    def __init__(self, dataframe, root_dir, sequence_length=10, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dataframe (DataFrame): DataFrame containing video filenames and labels.\n",
        "            root_dir (str): Directory path where video files are stored.\n",
        "            sequence_length (int): Number of frames to extract from each video.\n",
        "            transform (callable, optional): Optional transform to be applied on a frame.\n",
        "        \"\"\"\n",
        "        self.dataframe = dataframe\n",
        "        self.root_dir = root_dir\n",
        "        self.sequence_length = sequence_length\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        video_filename = self.dataframe.iloc[idx]['Filename']\n",
        "        video_path = os.path.join(self.root_dir, video_filename)\n",
        "        label = self.dataframe.iloc[idx]['label_value']\n",
        "        # Read video and extract frames\n",
        "        frames, _, _ = read_video(video_path, pts_unit='sec', start_pts=0, end_pts=10, output_format='TCHW')\n",
        "        total_frames = len(frames)\n",
        "        frame_indices = torch.linspace(0, total_frames - 1, steps=self.sequence_length).long()\n",
        "        selected_frames = frames[frame_indices]\n",
        "\n",
        "        processed_frames = []\n",
        "        for frame in selected_frames:\n",
        "            if self.transform:\n",
        "                frame = self.transform(frame)\n",
        "            processed_frames.append(frame)\n",
        "\n",
        "        frames_tensor = torch.stack(processed_frames)\n",
        "        return frames_tensor, label\n",
        "\n",
        "# Example of setting up the dataset and dataloader with transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),  # Necessary to convert raw video frame to PIL Image for some transformations\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((224,224)),\n",
        "    transforms.ToTensor(),  # Convert the PIL Image to a tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the tensor\n",
        "])\n",
        "\n",
        "dataset = VideoDataset(df_labels, DATASET_VIDEO_PATH, transform=transform)\n",
        "loader = DataLoader(dataset, batch_size=1, shuffle=True)"
      ],
      "metadata": {
        "id": "_r14Js6kIdiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Assuming 'df_labels' is your DataFrame containing video filenames and their labels\n",
        "# Shuffle the DataFrame\n",
        "df_labels = df_labels.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Define split sizes\n",
        "train_size = int(0.7 * len(df_labels))\n",
        "val_size = int(0.15 * len(df_labels))\n",
        "test_size = len(df_labels) - train_size - val_size\n",
        "\n",
        "# Split the DataFrame into train, validation, and test sets\n",
        "train_df = df_labels[:train_size]\n",
        "val_df = df_labels[train_size:train_size + val_size]\n",
        "test_df = df_labels[train_size + val_size:]\n",
        "\n",
        "# Define a transform pipeline\n",
        "transform = Compose([\n",
        "    ToPILImage(),  # Convert raw video frame to PIL Image for transformations\n",
        "    Resize((256, 256)),\n",
        "    CenterCrop((224, 224)),\n",
        "    ToTensor(),  # Convert the PIL Image to a tensor\n",
        "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize the tensor\n",
        "])\n",
        "\n",
        "# Create datasets for each set\n",
        "train_dataset = VideoDataset(train_df, DATASET_VIDEO_PATH, transform=transform)\n",
        "val_dataset = VideoDataset(val_df, DATASET_VIDEO_PATH, transform=transform)\n",
        "test_dataset = VideoDataset(test_df, DATASET_VIDEO_PATH, transform=transform)\n",
        "\n",
        "# Create data loaders for each set\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "# Now you can use these loaders in your training loop\n"
      ],
      "metadata": {
        "id": "XODSEJzNIfze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Assuming train_loader is already defined\n",
        "data_loader = train_loader  # You can replace this with val_loader or test_loader as needed\n",
        "\n",
        "# Fetch and print the shape of one batch of data and labels from the specified DataLoader\n",
        "for data, labels in data_loader:\n",
        "    print(f\"Data shape: {data.shape}\")  # Shape of the video frames tensor\n",
        "    print(f\"Labels shape: {labels.shape}\")  # Shape of the labels tensor\n",
        "    break  # Only look at the first batch\n"
      ],
      "metadata": {
        "id": "K09Qtq_BIjOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class VideoCNN3D(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VideoCNN3D, self).__init__()\n",
        "        # 3D convolutional layer with input channels = 3 (RGB)\n",
        "        self.conv1 = nn.Conv3d(3, 32, kernel_size=(3, 3, 3), padding=1)\n",
        "        self.pool1 = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
        "\n",
        "        # Second 3D convolutional layer\n",
        "        self.conv2 = nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=1)\n",
        "        self.pool2 = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
        "\n",
        "        # Third 3D convolutional layer\n",
        "        self.conv3 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=1)\n",
        "        self.pool3 = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
        "\n",
        "        # Flattening and dense layers\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(128 * 10 * 28 * 28, 512)  # Adjust size according to the output of the last pool layer\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(512, 2)  # Assuming binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Invert sequence length and channels dimensions\n",
        "        x = input_tensor.permute(0, 2, 1, 3, 4)\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.softmax(x, dim=1)\n",
        "        return torch.argmax(x, dim=1)\n",
        "\n",
        "# Example initialization and forward pass simulation\n",
        "model = VideoCNN3D()\n",
        "print(model)\n",
        "\n",
        "# Simulate a forward pass\n",
        "input_tensor = torch.randn(1, 10 ,3, 224, 224)  # Batch size, Channels, Sequence length, Height, Width\n",
        "output = model(input_tensor)\n",
        "print(\"Output shape:\", output.shape)\n"
      ],
      "metadata": {
        "id": "TWHJ0m_tImHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VideoCNN3D()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n"
      ],
      "metadata": {
        "id": "8SBlqfMAIobP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install av"
      ],
      "metadata": {
        "id": "U7Kci86nIr0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print('-' * 10)\n",
        "\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs = inputs\n",
        "            labels = labels\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        print(f'Training Loss: {epoch_loss:.4f}')\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs = inputs\n",
        "            labels = labels\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            val_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        print(f'Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "        # Deep copy the model if it has the best validation loss so far\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            print('Best model saved.')\n",
        "\n",
        "        # Step the scheduler\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "    print('Training complete. Best val Loss: {:4f}'.format(best_loss))\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "# Train the model\n",
        "trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, scheduler)\n"
      ],
      "metadata": {
        "id": "MA29rfRhIt-O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}